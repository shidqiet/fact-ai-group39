{"cells":[{"cell_type":"code","execution_count":null,"id":"8eca29c4","metadata":{"id":"8eca29c4"},"outputs":[],"source":["from transformers import AutoTokenizer\n","from datasets import load_dataset, Dataset, DatasetDict\n","import random"]},{"cell_type":"code","execution_count":null,"id":"d8d873bb","metadata":{"id":"d8d873bb"},"outputs":[],"source":["def clean_dataset():\n","    dataset = load_dataset(\"roneneldan/TinyStories\")\n","\n","    full = list(dataset[\"validation\"][\"text\"]) + list(dataset[\"train\"][\"text\"])\n","\n","    cleaned = [s.encode('ascii', 'ignore').decode('ascii') for s in full]\n","    deduped = list(set(cleaned))\n","    random.shuffle(deduped)\n","\n","    train, validation = deduped[2**14:], deduped[:2**14]\n","\n","    train_ds= Dataset.from_dict(dict(text=train))\n","    validation_ds = Dataset.from_dict(dict(text=validation))\n","\n","    dataset_dict = DatasetDict({\"train\": train_ds, \"validation\": validation_ds})\n","\n","    # Load tokenizer and tokenize the dataset\n","    tokenizer = AutoTokenizer.from_pretrained(\"tdooms/ts-tokenizer-4096\", pad_token=\"[EOS]\")\n","\n","    def tokenize_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=256)\n","\n","    tokenized_dataset = dataset_dict.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n","\n","    return tokenized_dataset"]},{"cell_type":"code","execution_count":null,"id":"3ae0256e","metadata":{"id":"3ae0256e","outputId":"c7e56300-ee8d-44b5-9b17-7be718a54145","colab":{"referenced_widgets":["dc2f0c33da7e4b86a9f5aef282e1495d","83262389d56a4feabc057dea960b03dd","4616488a92d843d4b47c1682a2af012d","8b354367e2b24448b2961f728002bdf2"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["Cleaning and tokenizing dataset...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc2f0c33da7e4b86a9f5aef282e1495d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1798254 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83262389d56a4feabc057dea960b03dd","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/16384 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving tokenized dataset to 'ts-tokenized-final'...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4616488a92d843d4b47c1682a2af012d","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/6 shards):   0%|          | 0/1798254 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b354367e2b24448b2961f728002bdf2","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/16384 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Done!\n"]}],"source":["print(\"Cleaning and tokenizing dataset...\")\n","tokenized_ds = clean_dataset()\n","\n","print(\"Saving tokenized dataset to 'ts-tokenized-final'...\")\n","tokenized_ds.save_to_disk(\"ts-tokenized-final\")\n","\n","print(\"Done!\")"]}],"metadata":{"kernelspec":{"display_name":"cv1","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}